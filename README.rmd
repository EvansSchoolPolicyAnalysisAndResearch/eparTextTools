---
output: html_document
---
# eparTextTools

This tookit provides a set of resources for analyzing textual documents using the R programming language for the conduct of portfolio analysis and review. The tools rely on text mining, natural language processing, and machine learning programs developed by other R users and as such heavily relies on code developed by other packages. Thus, it may be thought of as a set of tools enabling portfolio analysis rather than a new package for conduct of text analysis.

##Extracting Text

To begin this process, first load the textFunctions. For demonstration purposes we also create a demonstration document dataset using the demo.docs.R code.

```{r, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(message=FALSE, warnings=FALSE)
source("textFunctions.R")
source("demo.docs.R")
```

Main functions enabling workflow include reading documents into R via the "getTextR" command. The getTextR command takes a file directory as an argument, and returns a textual corpus. It is capable of reading word documents (both doc and docx), pdf documents, and txt documents. Further document types can be added by replacing the "FILETYPE NA" line with additional document if loops. The allDocs() command is a conveniance wrapper which loops through a directory parsing documents and creating a corpus with metadata.

```{r}
corpus1<-allDocs("demo.docs.folder")
```

Documents are read into R as a textual corpus--the method used by the TM package. This enables preservation of document metatdata alongside document text. Metadata such as ids and timestamps is stored in the .$meta location of each corpus.

```{r }
lapply(corpus1,function(X){X$meta})[[1]]
```

###Cleaning and Parsing Text
Texts are stored in the corpus as character strings facilitating use of different analysis packages. Some basic cleaning tools from the TM package may be helpful for performing various funcitons.

```{r}
corpus2<-doc_clean_process(corpus1)
```

The object Corpus2 is now a cleaned version of the original corpus, which is useful for many description tasks and some analysis tasks.

###Description: Word Frequencies and Associations

For basic description, it is often useful to generate a term document matrix. This is done via the code below, with an additional step to remove sparse terms.

```{r}
tdm<-TermDocumentMatrix(corpus2) %>% removeSparseTerms(.,.2)
```
Once a term document matrix is available, one can easily begin to create tables and charts to explore the data.

The word_heatmap function provides an easy wrapper for structuring term document matrices into an interactive matrix.

```{r, results='hide'}
hm<-word_heatmap(tdm,6)
```

Using wfplots(), one can create a basic ggplot() object describing the most frequent terms across documents, or the documents in which the most frequent terms are likely to occur. The objects created by the command can be edited by adding on additional functions.

```{r, results='hide'}
p1<-wfplots(tdm,typePlot=1,10)
```{r, echo=FALSE}
p1
```
```{r, results='hide'}
p2<-wfplots(tdm,typePlot=0,10)
```
```{r, echo=FALSE}
p2
```
```{r, results='hide'}
p3<-wfplots(tdm,typePlot=0,10)+theme_fivethirtyeight()+facet_wrap(~word)
```
```{r, echo=FALSE}
p3
```

The same is true for the interest_plot command, which allows the user to specify words they are interested in viewing across documents rather than relying on specific frequencies. 

```{r,results='hide'}
p4<-interest_plot(c("staff","meet","organ"),tdm,by.var=c("Programmatic","Programmatic","Subject"),"Word Categories")
```{r,echo=FALSE}
p4
```
```{r,results='hide'}
p5<-interest_plot_bydoc(c("staff"),tdm)
```
```{r,echo=FALSE}
p5
```
```{r, results='hide'}
p6<-interest_plot_bydoc(c("staff","meet"),tdm)
```
```{r,echo=FALSE}
p6
```



By editing the term Document matrix to include weighting, each of these commands can be used while taking the length of documents into account.

```{r,results='hide'}
tdm2<-TermDocumentMatrix(corpus2,control=list(weighting=function(x) weightSMART(x)))
p7<-wfplots(tdm2,typePlot=1,10)
```

```{r, echo=FALSE}
p7
```

```{r,results='hide'}
p8<-wfplots(tdm2,typePlot=0,10)
```

```{r, echo=FALSE}
p8
```

```{r, results='hide'}
p9<-interest_plot_bydoc(c("school"),tdm2)
p10<-word_heatmap(tdm2,6)
tdm3<-tdm
tdm3$v<-(tdm3$v/colSums(as.matrix(tdm3))[tdm3$j])*100
p11<-wfplots(tdm3,typePlot=1,2)
p12<-word_heatmap(tdm3,6)
```

```{r, echo=FALSE}
p9
p10
p11
p12
```

One command, assocPrettyOneStep(), takes a wordlist as an argument and returns a list of associated words above a correlation threshold. This thus informs what words are most likely to cooccur accross a corpus of documents.

```{r, warning=FALSE}
assocPrettyOneStep(c("develop","target"),tdm, corpus2,.5)
```

While the functions to tokenization based on individual words, bigrams or trigrams can also be used to process text.

```{r}
BigramTokenizer <-function(x){unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)}
#freqterms<-lapply(1:length(corpus2), function(X){findFreqTerms(tdm_bi[,X],lowfreq=10)})
#freqterms
```

###Categorization

####Topic Modeling and Document Clustering 

###Exploration and Model Identification

####Natural Language Processing

####Supervised/Unsupervised Learning
