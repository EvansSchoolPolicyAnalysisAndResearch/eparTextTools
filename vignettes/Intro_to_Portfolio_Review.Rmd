---
title: "An Introduction to Computer Assisted Portfolio Review"
author: "Ryan P Scott"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{An Introduction to Computer Assisted Portfolio Review}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This tookit provides a set of resources for analyzing textual documents using the R programming language for the conduct of portfolio analysis and review. The tools rely on text mining, natural language processing, and machine learning programs developed by other R users and as such heavily relies on code developed by other packages. Thus, it may be thought of as a set of tools combining other packages and thus enabling portfolio review rather than a new package for conduct of text analysis. 

## Introduction

  Granting agencies, foundations, and donors often possess large quantities of textual data which they are unable to quickly and efficiently operationalize into meaningful evidence about theories of program change. After a foundation has spent millions of dollars on hundreds of projects, in hundreds of locations, employing thousands of different indicators and outcome measures, based on variously well-articulated theories of change, it is impossible with any reasonable level of confidence to 1. estimate the collective impact of a portfolio of investments within a foundation; 2. estimate the collective impact of investments across foundations; and 3. know that non-consenting individuals have not been made worse off (despite good intent). This is a problem of measurement and accountability for foundations and the public. 
  
Portfolio management is a common method for organizing an investment strategy. Portfolio management allows publics and foundations to identify assets and funding amounts connect spending to theories and outcomes (Sleet, Moffett, and Stevens 2008). Approaches to portfolio management such as the Balanced Scorecard have been useful in directing private and public financial investments in a manner that aligns financial goals with mission goals (Robert S. Kaplan 2001; Hoque 2014/3). These tools aim to connect programs to specific objectives and measures, providing organizations with clear and measureable objectives across multiple value areas. Transparency can help foundations to confront commonly identified missteps of public foundations including mission ambiguity (Eisenberg 1984) and institutional myth (Meyer and Rowan 1977), However, use of such evaluative tools tend to be one-off, tailor made to individual foundations, used to justify spending, or adopted by only the largest research foundations (Hyndman and McConville 2016); (Sleet, Moffett, and Stevens 2008; Schmitz and Schillo 2005; Horton and Mackay 2003). 
   
Portfolio management can either be prospective and retrospective, helping to plan future activities strategies or technologies in light of current assets and risks (Oliveira and Rozenfeld 2010; R. S. Kaplan and Norton 1995). We term the application of portfolio assessment to characterize past actions and assessments with the purpose of influencing prospective management portfolio review. However, portfolio review is limited within foundations. Previously, large organizations possessing diverse funding areas have been challenged by a lack of tools for scaling portfolio reviews across widely differing domains of investment as well as by cognitive and practical limitations of the boundedly rational actors who must run such organizations (Selznick 1996; Simon 1972). Faced with limited time, diverse mission goals, and thousands of objectives, analyzing a portfolio of projects is simply too time consuming, costly, and threatening for program managers who, given limited time frames of employment, must optimize resource allocation not necessarily return on investment. 
  
Current portfolio review methods rely on hand coding of project documentsâ€”a process that takes countless manhours and limits the ability of foundations to apply porfolio review in adaptive decision making settings. As a result, foundations need better tools for analyzing spending and performance information across portfolios. This includes developing methodologies for portfolio review that are cost effective, scaleable, and rapidly deployable so that foundations can learn from current and past investments, comparing spending and impact against expectations and against each other in order to target funding to programs with proven effectiveness, identify gaps in theory, and exploit under-utilized opportunities.
    
However, the field of public administration is increasingly adopting the use of text mining methodologies and computational social scientific approaches for characterizing large textual databases. These methods provide an avenue for scaling portfolio review to a much wider array of organizations who traditionally lacked resources for conduct of retrospective portfolio management strategies.
   
We propose and demonstrate the use of data analytic tools including natural language processing and both supervised and unsupervised machine learning to rapidly characterize funding documents identifying theories of change, tying these theories of change to funding levels, and identifying how funding of specific interventions resulted in changes in outcomes of concern to various foundations. The novelty of this new approach is we plan to utilize data analytic tools to automate many of the above processes, providing a manner of reducing the labor and time costs facing a foundation hoping to conduct a portfolio review. Thus, while our methodologies are not novel, nor are portfolio reviews new concepts, we believe that the application of data analytic tools within foundation portfolio review has the potential to dramatically improve the connection between theory and investment for public organizations. Natural language processing and machine learning are common tools for language extraction and coding across a growing range of fields including health, economics, and political science (Carvalho, Freitas, and da Silva 2013; Turian 2013; Wilkerson, Smith, and Stramp 2013; Lee 2000). Specifically, tools allowing for extraction of relations between words are promising for portfolio review because they allow connecting actors, actions, and outcomes identified within programmatic documents to corresponding investments (Turian 2013).

## Portfolio Review Focus

We begin by addressing the basic question, "how have the research projects conducted by the Evans School Policy Analysis and Research (EPAR) group informed the three aims of the research foci, specifically gender, adoption, and measurement, and what potential synergies exist across these current research topics which may inform the comparative advantage of the EPAR research group?"

Within each of the three-research foci, we demonstrate how the tools we are developing can be applied for addressing a key question that may face an organization like ours.

The package can be installed by using the command,
```{r,eval=FALSE}
devtools::install_github("ryscott5/epartexttools")
```

```{r}
options(java.parameters = "-Xmx4g")
library(epartexttools)
```

##Planned method of study
## Q1 What research targets gender, adoption, and measurement?

We begin by reading in a corpus. The EPAR demonstration corpus can be read in and downloaded using the example_documents command which will create a folder titled "demo.docs.folder" in the current working directory. The downloaded documents can then be loaded usingthe allDocs() command. Documents are read into R as a textual corpus--the method used by the TM package. This enables preservation of document metatdata alongside document text. Metadata such as ids and timestamps is stored in the .$meta location of each corpus.


```{r, echo=TRUE, results='hide'}
example_documents()
corpus1<-allDocs(directory="demo.docs.folder",SkiponError=FALSE)
jgc()
```
  
Currently, allDocs supports parsing of txt, pdf,docx, doc, or txt files. However, it is itself a wrapper which reads a directory files using the getTextR function, which itself utilizes modified versions of the read_doc, readPDF, or readPlain functions from the tm package, and the read_docx function from qdapTools. Once documents are read in, the metadata from the documents can be extracted by loooping through elements included within the document, for example;

```{r, echo=TRUE, results='hide',eval=FALSE}
lapply(corpus1,function(X){X$meta})[[1]]
```

This method of reading in documents treats all documents as unstructured and keeps all elements of text included within a document. It is thus useful for characterizing batches of documents for which there is no common structure. If a common structure does exist for documents, then elements of a document can be extracted using pattern matching (via stringr) or via the tools described in the "Structured Document Analysis" vignette. 

###Cleaning and Parsing Text
Texts are stored in the corpus as character strings facilitating use of different analysis packages. For addressing the first research question-- "What research targets gender, adoption and measurement?" a basic text mining approach that relies on word frequencies, counts, and occurances of words may be optimal. For this, we may want to perform common tasks such as removing whitespace, eliminating stopwords, removing punctuation, and stemming documents. All of these are accomplished rapidly by the doc_clean_process() command which streamlines functionality from the tm package for rapid portfolio review. The second command, TermDocumentMatrix creates a word frequency matrix and removes sparse terms allowing graphical and exploratory analysis of text.

```{r, echo=TRUE, results='asis'}
corpus2<-doc_clean_process(corpus1)
tdm<-TermDocumentMatrix(corpus2) %>% removeSparseTerms(.,.6)
```


To begin exploring documents, we highlight two commands which provide useful interfaces for seaching documents and clustering documents. When exploring a textual corpora, one often has some sort of an idea of what kinds of information they are interested in. For exa,ple, for the above questions we might be interested in what documents discuss terms such as women, gender, access.

The function wordcount_table() is a useful starting point-- it allows us to search for word frequencies in documents, pulling counts of how many times a word occurs in various documents, and weighting that count by the length of the document if we so desire.

```{r,echo=TRUE, results='asis'}
wordcount_table(c("gender","access"),tdm,corpus1)
```

The function generates an html table which can be saved using the saveWidget() command or which can be used just to interact with the data. By default, the function TermDocumentMatrix weights according to weightTF which just counts term frequencies. We might instead want to weight by the term frequency relative to the document frequency, as below. In this command, rather then generating an interactive table, setting raw=true generates a data.table which can then be manipulated within R.


```{r,echo=TRUE, results='asis'}
tout<-wordcount_table(c("gender","access"),tm::TermDocumentMatrix(corpus2,control=list(weighting=function(X) tm::weightTfIdf(X, normalize=FALSE))),corpus1,raw=T)
head(tout[,1:3])
```

For clustering we can utilize the d3heatmap package to construct a heatmap of which words are the most common (y axis) across a set of documents (x axis) where the user can select the number of words they are interested in (6 in demo).

```{r,echo=TRUE, results='asis'}
word_heatmap(tdm,6)

word_heatmap(tdm,pickwords=c("women","gender","access","land","right","work","labor","yield","security"))
```

The tdm object can be edited to narrow the graphical information presented, for example if we are only interested words within documents which contain the word women, we can use piping to structure the tdm for the graph. The following code creates a heatmap for documents where gender and women both occur at least once, but then clusters those documents based on the 20 most common words across the entire corpus. 

```{r,echo=TRUE, results='asis'}
tdm[,as.vector(tdm["gender",])>1] %>% .[,as.vector(.["women",])>1] %>% word_heatmap(.,20)
```

If static figures are needed, with wfplots(), one can create a basic ggplot() object describing the most frequent terms across documents, or the documents in which the most frequent terms are likely to occur. The objects created by the command can be edited by adding on additional functions, by filtering the term document matrix, or by changing the typePlot. 

```{r}
wfplots(tdm[,c(2,10,12)],typePlot=2, 5,shortendoc=TRUE)
```

```{r}
tdm[,as.vector(tdm["gender",])>20] %>% .[,as.vector(.["women",])>10] %>% wfplots(.,typePlot=2,10,shortendoc=TRUE)
```

```{r}
tdm[,as.vector(tdm["gender",])>20] %>% .[,as.vector(.["women",])>10] %>% wfplots(.,typePlot=1,10,shortendoc=TRUE)+ggtitle("Change X versus Y")
```

```{r}
interest_plot_bydoc(c("women","farmer","school"),tdm[,1:5])+coord_flip() 
```

```{r}
interest_plot_bydoc(c("women","farmer","school"),tdm[,1:5]) %>% plotly::ggplotly() 
```

By editing the term document matrix to include weighting, each of these commands can be used while taking the length of documents into account.

```{r}
TermDocumentMatrix(corpus2[1:10],control=list(weighting=function(x) weightSMART(x))) %>% interest_plot_bydoc(c("women","farmer","school"),.) %>% plotly::ggplotly() 
```

These approaches can provide useful exploration, but to get at the key research questions, it can be beneficial to begin to explore how documents are related. If we are interested in identifying research that targets gender, adoption, and measurement respectively, we might first start by exploring words related to gender. 

```{r}
jgc()
assocPrettyOneStep("gender",tdm, corpus2,.5)
```

Based on the table, we can observe that the word "equal" is strongly associated with the word gender. We might use this information to cluster our term document matrix and compare documents where "gender" and "equal" occur frequently to other documents.

```{r}
tornadoCompare(tdm,c("gender","equal","femal"),3,10)
```

Based on this, we might notice that seasons and school are relatively more frequent for research projects targetting gender as are system related studies and studies of households.


##Document Clustering

Within text analysis, document clustering refers to the grouping of documents into a set of categories based on their semantic concepts. From a program evaluation or portfolio review perspective clustering documents is useful because it can allow us to move beyond user-generated queries to allowing a set of textual data provide evidence about underlying theories of change or policy processes.

A starting question, however, is the unit of analysis at which we would consider a policy theory to be operating at. On the one hand, each document may represent a theory. More than likely however, within a given document or proposal description, more than one theory of change or pathway will be described. Clustering documents in a way that assigns each proposal to a single categegory or cluster would ignore this potential variation. Luckily for the purposes of portfolio review, we are not the first to recognize that bodies of text are not monolithic entities! in fact, a whole series of text analytic progams have been develop that attempt to understand the semantic meanings within text, the connections between areas of text, and how near and far apart bodies of text are associated. 

As a start, it is useful to step back and reconsider what a "document" entails. While up to this point we have considered each unique PDF, docx, or doc as a "document" for study, taking a liberal approach to the word "document" will enable a more fine-grained approach to text analysis. For example, a document could refer to a sentence or a paragraph. Or inversely, a document could refer to a group of documents which are all part of one proposal. As such, the term document can be somewhat confusing!

For the purposes of clarity, it is thus easier to think about a set of documents as a set of strings or a set of character vectors. For each string--which might represent an entire proposal or a just a sentence from a proposal depending on the question at hand-- a researcher might decide on a set number of variables, and they could construct a spreadsheet where in one column are a set of strings, and in the other columns are features describing those strings such as funding agency, year, author, grant status, etc. Just as in an introductory statistics class the unit of anlysis is the rows in the spreadsheet while the columns represent variable. Text is just one variable we might consider in an analysis. Moreover, we might aggregate or disagregate text into bigger or larger chunks thus changing the unit of anlysis.

When we break text into smaller chunks it is generally refered to as tokenization. Joining segments of text is concatonization. You have already liberally applied tokenization in the excersizes above even if you did not realize it at the time. When we count word frequencies, we are tokenizing text into a words. We then count of often each word occurs in each document--this is really no different than seeing how many people live in a certain county within a dataset or attmepting to count how many farmers adopted a certain seed variety. 

While tokenization into words is great, we often want to know how various words go together and how words form sentences that produce meaning and finally (eventually) causal processses.

##Other approaches

There are many approaches to these challenges with R. an emerging method is TidyText which was just emerging at the time this project was starting http://tidytextmining.com/index.html 


##our approach
1. break documents into paragraphs
2. model the topics within paragraphs
3. cluster words,paragraphs, documents
4. extract relevatn causal patterns



```{r,eval=F}
jgc()
rm(corpus2)
rm(tdm)
workingfolder<-file.path("Research.Grants")
dir.create(file.path(workingfolder))
saveRDS(corpus1,file.path(workingfolder,"corpus.rds"))
rm(corpus1)
```

```{r,eval=FALSE}
corpus1<-readRDS(file.path(workingfolder,"corpus.rds"))
jgc()
gc()
BASE_INPUT<-PreTopicFrame(corpus1,15)
BASE_INPUT$out$meta$OpID<-BASE_INPUT$out$meta$Orig
#saves files so you can reload
jgc()
saveRDS(BASE_INPUT,file.path(workingfolder,"base_input1.rds"))
```

###Adding geographic information
```{r, eval=FALSE}
buildcliff()
startcliff()
library(RCurl)
library(httr)
BASE_INPUT$SentFrame$OpID<-BASE_INPUT$SentFrame$Orig
pred1<-PredictCountryByDoc(BASE_INPUT)
stopcliff()
BASE_INPUT$out$meta<-reflectCountryCol(BASE_INPUT$out$meta,pred1,10,FALSE)
getwd()
saveRDS(BASE_INPUT,file.path(workingfolder,"base_input1.rds"))
write.csv(pred1,file.path(workingfolder,"countrypredictions1.csv"))
jgc()
```

```{r,eval=FALSE}
library(plotly)
runMap(file.path(workingfolder,"countrypredictions1.csv"),path.file=T,"countries")
```

```{r,eval=FALSE}
writeFormulaforSTM(BASE_INPUT,workingfolder)
jgc()
```

```{r,eval=FALSE}
runSTM(workingfolder)
```


## Q2 Where is there alignment between causal pathways identified withins the three research foci?


## Q3 What are unique causal pathways identified within each of the three research foci?


## Q4 What causal pathways exist outside of the domains of gender, adoption and measurement that could be potentially integrated into the EPAR research strategy?


#Method

###Q1: Topic Modeling and Human Tagging

###Q2: Network Correlation

###Q3: Network Correlation

###Q4: Topic Modelling and Semantic Network Mapping

