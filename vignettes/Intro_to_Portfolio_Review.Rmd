---
title: "An Introduction to Computer Assisted Portfolio Review"
author: "Ryan P Scott"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_width: 7
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{An Introduction to Computer Assisted Portfolio Review}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This tookit provides a set of resources for analyzing textual documents using the R programming language for the conduct of portfolio analysis and review. The tools rely on text mining, natural language processing, and machine learning programs developed by other R users, and as such heavily relies on code developed by other packages. Thus, it may be thought of as a set of tools combining other packages and thus enabling portfolio review rather than a new package for conduct of text analysis. 

## Introduction

  Granting agencies, foundations, and donors often possess large quantities of textual data which they are unable to quickly and efficiently operationalize into meaningful evidence about theories of program change. After a foundation has spent millions of dollars on hundreds of projects, in hundreds of locations, employing thousands of different indicators and outcome measures, based on variously well-articulated theories of change, it is impossible with any reasonable level of confidence to 1. estimate the collective impact of a portfolio of investments within a foundation; 2. estimate the collective impact of investments across foundations; and 3. know that non-consenting individuals have not been made worse off (despite good intent). This is a problem of measurement and accountability for foundations and the public. 
  
Portfolio management is a common method for organizing an investment strategy. Portfolio management allows publics and foundations to identify assets and funding amounts connect spending to theories and outcomes (Sleet, Moffett, and Stevens 2008). Approaches to portfolio management such as the Balanced Scorecard have been useful in directing private and public financial investments in a manner that aligns financial goals with mission goals (Robert S. Kaplan 2001; Hoque 2014/3). These tools aim to connect programs to specific objectives and measures, providing organizations with clear and measureable objectives across multiple value areas. Transparency can help foundations to confront commonly identified missteps of public foundations including mission ambiguity (Eisenberg 1984) and institutional myth (Meyer and Rowan 1977), However, use of such evaluative tools tend to be one-off, tailor made to individual foundations, used to justify spending, or adopted by only the largest research foundations (Hyndman and McConville 2016); (Sleet, Moffett, and Stevens 2008; Schmitz and Schillo 2005; Horton and Mackay 2003). 
   
Portfolio management can either be prospective and retrospective, helping to plan future activities strategies or technologies in light of current assets and risks (Oliveira and Rozenfeld 2010; R. S. Kaplan and Norton 1995). We term the application of portfolio assessment to characterize past actions and assessments with the purpose of influencing prospective management portfolio review. However, portfolio review is limited within foundations. Previously, large organizations possessing diverse funding areas have been challenged by a lack of tools for scaling portfolio reviews across widely differing domains of investment as well as by cognitive and practical limitations of the boundedly rational actors who must run such organizations (Selznick 1996; Simon 1972). Faced with limited time, diverse mission goals, and thousands of objectives, analyzing a portfolio of projects is simply too time consuming, costly, and threatening for program managers who, given limited time frames of employment, must optimize resource allocation not necessarily return on investment. 
  
Current portfolio review methods rely on hand coding of project documentsâ€”a process that takes countless manhours and limits the ability of foundations to apply porfolio review in adaptive decision making settings. As a result, foundations need better tools for analyzing spending and performance information across portfolios. This includes developing methodologies for portfolio review that are cost effective, scaleable, and rapidly deployable so that foundations can learn from current and past investments, comparing spending and impact against expectations and against each other in order to target funding to programs with proven effectiveness, identify gaps in theory, and exploit under-utilized opportunities.
    
However, the field of public administration is increasingly adopting the use of text mining methodologies and computational social scientific approaches for characterizing large textual databases. These methods provide an avenue for scaling portfolio review to a much wider array of organizations who traditionally lacked resources for conduct of retrospective portfolio management strategies.
   
We propose and demonstrate the use of data analytic tools including natural language processing and both supervised and unsupervised machine learning to rapidly characterize funding documents identifying theories of change, tying these theories of change to funding levels, and identifying how funding of specific interventions resulted in changes in outcomes of concern to various foundations. The novelty of this new approach is we plan to utilize data analytic tools to automate many of the above processes, providing a manner of reducing the labor and time costs facing a foundation hoping to conduct a portfolio review. Thus, while our methodologies are not novel, nor are portfolio reviews new concepts, we believe that the application of data analytic tools within foundation portfolio review has the potential to dramatically improve the connection between theory and investment for public organizations. Natural language processing and machine learning are common tools for language extraction and coding across a growing range of fields including health, economics, and political science (Carvalho, Freitas, and da Silva 2013; Turian 2013; Wilkerson, Smith, and Stramp 2013; Lee 2000). Specifically, tools allowing for extraction of relations between words are promising for portfolio review because they allow connecting actors, actions, and outcomes identified within programmatic documents to corresponding investments (Turian 2013).

## Portfolio Review Focus

We begin by addressing the basic question:

***How have the research projects conducted by the Evans School Policy Analysis and Research (EPAR) group informed the three aims of the research foci, specifically gender, adoption, and measurement?***

Second,

***what potential synergies exist across these current research topics which may inform the comparative advantage of the EPAR research group?***

Within each of the three-research foci, we demonstrate how the tools we are developing can be applied for addressing a key question that may face an organization like ours.

##Installing the package.

Because many of the tools run here take time and depend on externally installed packages, it might be easier to run the code on an external server rather than working to ensure every package is installed at your workstation. To facilitate your setting up a server, you can find directions for how to set up a Google Cloud Compute insance at [this link](http://htmlpreview.github.io/?https://raw.githubusercontent.com/ryscott5/eparTextTools/master/vignettes/GoogleDirections.html).

The package, epartexttools, can be installed by using the command,
```{r,eval=FALSE}
devtools::install_github("ryscott5/epartexttools")
```

Prior to loading the package, it is very important to first set the java parameters to a larger "swap" size which is done via the following code, thus, prior to calling library, the code below expands the java limit to 4 GB. 

```{r}
options(java.parameters = "-Xmx4g")
library(epartexttools)
```

##Processing Documents

We begin by reading in a corpus. The EPAR demonstration corpus can be read in and downloaded using the example_documents command which will create a folder titled "demo.docs.folder" in the current working directory. The downloaded documents can then be loaded usingthe allDocs() command. Documents are read into R as a textual corpus--the method used by the TM package. This enables preservation of document metatdata alongside document text. Metadata such as ids and timestamps is stored in the .$meta location of each corpus.

```{r, echo=TRUE, results='hide', eval=F}
example_documents()
corpus1<-allDocs(directory="demo.docs.folder",SkiponError=FALSE)
```

```{r}
#Make directory for saving and save a copy of the corpus
epartexttools::makeworking("Research.Grants")
```

```{r, eval=F}
saveRDS(corpus1,file.path(workingfolder, "corpus.rds"))
```

```{r, echo=T, eval=T}
corpus1<-readRDS(file.path(workingfolder,"corpus.rds"))
```

Currently, the allDocs  command supports parsing of txt, pdf,docx, doc, or txt files. However, it is itself a wrapper which reads a directory files using the getTextR function, which itself utilizes modified versions of the read_doc, readPDF, or readPlain functions from the tm package, and the read_docx function from qdapTools. Once documents are read in, the metadata from the documents can be extracted by loooping through elements included within the document, for example;

```{r, echo=TRUE}
lapply(corpus1,function(X){X$meta})[[1]]
```

This method of reading in documents treats all documents as unstructured and keeps all elements of text included within a document. It is thus useful for characterizing batches of documents for which there is no common structure. If a common structure does exist for documents, then elements of a document can be extracted using pattern matching (via stringr) or via the tools described in the "Structured Document Analysis" vignette. 


When beginning a text analysis process, a good starting point is consideration of the is the unit of analysis which we will use for a study. On the one hand, each document may represent a theory of policy change or for a program under study. More than likely however, within a given document or proposal description, more than one theory of change or pathway will be described. Clustering documents in a way that assigns each proposal to a single categegory or cluster would ignore this potential variation. Luckily for the purposes of portfolio review, we are not the first to recognize that bodies of text are not monolithic entities! in fact, a whole series of text analytic progams have been develop that attempt to understand the semantic meanings within text, the connections between areas of text, and how near and far apart bodies of text are associated. 

As a start, it is useful to step back and reconsider what a "document" entails. While up to this point we have considered each unique PDF, docx, or doc as a "document" for study, taking a liberal approach to the word "document" will enable a more fine-grained approach to text analysis. For example, a document could refer to a sentence or a paragraph. Or inversely, a document could refer to a group of documents which are all part of one proposal. As such, the term document can be somewhat confusing!

For the purposes of clarity, it is thus easier to think about a set of documents as a set of strings or a set of character vectors. For each string--which might represent an entire proposal or a just a sentence from a proposal depending on the question at hand-- a researcher might decide on a set number of variables, and they could construct a spreadsheet where in one column are a set of strings, and in the other columns are features describing those strings such as funding agency, year, author, grant status, etc. Just as in an introductory statistics class the unit of anlysis is the rows in the spreadsheet while the columns represent variable. Text is just one variable we might consider in an analysis. Moreover, we might aggregate or disagregate text into bigger or larger chunks thus changing the unit of anlysis.

When we break text into smaller chunks it is generally refered to as tokenization. Joining segments of text is concatonization.  When we count word frequencies, we are tokenizing text into a words. We then count of often each word occurs in each document--this is really no different than seeing how many people live in a certain county within a dataset or attmepting to count how many farmers adopted a certain seed variety. 

###Cleaning and Parsing Text
Within R texts are stored in the corpus as character strings facilitating use of different analysis packages. While we can break those texts into smaller segements or expand them into larger segments, we often want to do so in a systematic way. To begin, we will explore texts with the unit of analysis being individual reports which were downloaded to build the demo dataset. Each report is a study from the EPAR Website.



While we could take the texts as in, there are lots of features in text we generally dont care much about--blank lines, periods, capitalization, white spaces, margins, random-chunks-of-meaningless-text. As a result, we *may* want to perform common tasks such as removing whitespace, eliminating stopwords (boring words), removing punctuation, and stemming (taking words down to their roots). All of these are accomplished rapidly by the doc clean process  command which streamlines functionality from the tm package for rapid portfolio review. It tokenizes documents into words and then stems words before joining them back into a a corpus. 

A second command, TermDocumentMatrix, creates a word frequency matrix and removes sparse terms allowing graphical and exploratory analysis of text. A Term Document Matrixes structures a group of text into a matrix where for every word there is a count of how often that word occurs wihtin a given document. Again, remember, our unit of analysis is going to be a report, so  we can use a term document matrix to study how term usage varies across those reports. A Term Document Matrix, however, stores, documents in columns and words in each row. Notice, the unit of anlysis could thus be either words (with counts in each report) or reports (which counts of words in each). 

```{r, echo=TRUE, results='asis', eval=FALSE}
#cleans elements of corpus
corpus2<-doc_clean_process(corpus1)
saveRDS(corpus2,file.path(workingfolder,"corpus_cleaned.rds"))
```

```{r,echo=FALSE}
corpus2<-readRDS(file.path(workingfolder,"corpus_cleaned.rds"))
```


```{r}
#converts corpus into term document matrix, removing terms that occur infrequently (can be adjusted by manipulating that .6)
tdm<-TermDocumentMatrix(corpus2) %>% removeSparseTerms(.,.8)
```


##Counting, describing, and mining for what research targets gender, adoption, and measurement?


To begin exploring documents, we highlight two commands which provide useful interfaces for seaching documents and clustering documents. When exploring a textual corpora, one often has some sort of an idea of what kinds of information they are interested in. For example, for the above questions we might be interested in what documents discuss terms such as women, gender, access.

The function wordcount_table() is a useful starting point-- it allows us to search for word frequencies in documents, pulling counts of how many times a word occurs in various documents, and weighting that count by the length of the document if we so desire.

```{r,echo=TRUE, results='asis'}
#Searches corpus 1 for the words gender and access based and returns counts based on the term document matrix we built above.
wordcount_table(c("gender","access"),tdm,corpus1)
```

The function generates an html table which can be saved using the saveWidget() command or which can be used just to interact with the data. By default, the function TermDocumentMatrix weights according to weightTF which just counts term frequencies. We might instead want to weight by the term frequency relative to the document frequency, as below. In this command, rather then generating an interactive table, setting raw=true generates a data.table which can then be manipulated within R.


```{r,echo=TRUE, results='asis'}
tout<-wordcount_table(c("gender","access"),tm::TermDocumentMatrix(corpus2,control=list(weighting=function(X) tm::weightTfIdf(X, normalize=FALSE))),corpus1,raw=T)
head(tout[,1:3])
```


The tdm object can be edited to narrow the graphical information presented, for example if we are only interested words within documents which contain the word women, we can use piping to structure the tdm for the graph. The following code creates a heatmap for documents where gender and women both occur at least once, but then clusters those documents based on the 20 most common words across the entire corpus. 

```{r,echo=TRUE, results='asis'}
tdm[,as.vector(tdm["gender",])>1] %>% .[,as.vector(.["women",])>1] %>% word_heatmap(.,20)
```

If static figures are needed, with wfplots(), one can create a basic ggplot() object describing the most frequent terms across documents, or the documents in which the most frequent terms are likely to occur. The objects created by the command can be edited by adding on additional functions, by filtering the term document matrix, or by changing the typePlot. 

```{r}
wfplots(tdm[,c(2,10,12)],typePlot=2, 5,shortendoc=TRUE)
```

```{r}
tdm[,as.vector(tdm["gender",])>20] %>% .[,as.vector(.["women",])>10] %>% wfplots(.,typePlot=2,10,shortendoc=TRUE)
```

```{r}
tdm[,as.vector(tdm["gender",])>20] %>% .[,as.vector(.["women",])>10] %>% wfplots(.,typePlot=1,10,shortendoc=TRUE)+ggtitle("Change X versus Y")
```

```{r}
interest_plot_bydoc(c("women","farmer","school"),tdm[,1:5])+coord_flip() 
```

```{r}
interest_plot_bydoc(c("women","farmer","school"),tdm[,1:5]) %>% plotly::ggplotly() 
```

By editing the term document matrix to include weighting, each of these commands can be used while taking the length of documents into account.

```{r}
TermDocumentMatrix(corpus2[1:10],control=list(weighting=function(x) weightSMART(x))) %>% interest_plot_bydoc(c("women","farmer","school"),.) %>% plotly::ggplotly() 
```



While tokenization into words is great, we often want to know how various words go together and how words form sentences that produce meaning and finally (eventually) causal processses.

Within text analysis, document clustering refers to the grouping of documents into a set of categories based on their semantic concepts. From a program evaluation or portfolio review perspective clustering documents is useful because it can allow us to move beyond user-generated queries to allowing a set of textual data provide evidence about underlying theories of change or policy processes.

For initial clustering of text, we can utilize the d3heatmap package to construct a heatmap of which words are the most common (y axis) across a set of documents (x axis) where the user can select the number of words they are interested in (6 in demo).

```{r,echo=TRUE, results='asis'}
word_heatmap(tdm,6)

word_heatmap(tdm,pickwords=c("women","gender","access","land","right","work","labor","yield","security"))
```

Perhaps, alternatively we would like to know which words are most highly associated with another word. Here, we generate a table where we show words which occur most commonly with the word gender within the corpus.

```{r}
assocPrettyOneStep("gender",tdm, corpus2,.5)
```

Based on the table, we can observe that the word "equal" is strongly associated with the word gender. We might use this information to cluster our term document matrix and compare documents where "gender" and "equal" occur frequently to other documents.

```{r}
tornadoCompare(tdm,c("gender","equal","femal"),3,10)
```

Based on this, we might notice that seasons and school are relatively more frequent for research projects targetting gender as are system related studies and studies of households.

##Document Clustering

In the above analysis, there was a brief intro into document clustering for the creation of the heatmap of words. However, clustering of documents and words is the basis of much of the automated portfolio review toolset and so we will go into it in more detail throughout the rest of the vignette.

#Structuring documents for topic modeling

While up to this point we have taken documents as given, most text analytic methods were developed for smaller chunks of text. Accordingy, it can be useful to break texts into paragraphs or sentences and then to consider the origin document as a covariate operating on that text chunk. We can thus think of the origin document as a categorical factor that has some unknown relationship to the sentence or text chunk. In the code below, we break a corpus of text--names corpus 1, into paragraph chunks preserving the metadata from each previous chunk. In this code we use the pretopicframe2 function which relies on the Parsey McParseface parser.

```{r,eval=FALSE}
BASE_INPUT<-PreTopicFrame2(corpus1,workingfolder=workingfolder)
```


BASE_INPUT$out$meta$OpID<-BASE_INPUT$out$meta$Orig
#saves files so you can reload
saveRDS(BASE_INPUT,file.path(workingfolder,"base_input1.rds"))
```

###Adding geographic information
```{r, eval=FALSE}
buildcliff()
startcliff()
library(RCurl)
library(httr)
BASE_INPUT$SentFrame$OpID<-BASE_INPUT$SentFrame$Orig
pred1<-PredictCountryByDoc(BASE_INPUT)
stopcliff()
BASE_INPUT$out$meta<-reflectCountryCol(BASE_INPUT$out$meta,pred1,10,FALSE)
getwd()
saveRDS(BASE_INPUT,file.path(workingfolder,"base_input1.rds"))
write.csv(pred1,file.path(workingfolder,"countrypredictions1.csv"))
```

```{r,eval=FALSE}
library(plotly)
runMap(file.path(workingfolder,"countrypredictions1.csv"),path.file=T,"countries")
```

```{r,eval=FALSE}
writeFormulaforSTM(BASE_INPUT,workingfolder)
```

```{r,eval=FALSE}
runSTM(workingfolder)
```


## Q2 Where is there alignment between causal pathways identified withins the three research foci?


## Q3 What are unique causal pathways identified within each of the three research foci?


## Q4 What causal pathways exist outside of the domains of gender, adoption and measurement that could be potentially integrated into the EPAR research strategy?


#Method

###Q1: Topic Modeling and Human Tagging

###Q2: Network Correlation

###Q3: Network Correlation

###Q4: Topic Modelling and Semantic Network Mapping

